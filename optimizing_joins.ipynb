{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41192ad3-3e35-4a1b-acad-775b886ae003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d97ae5-c43b-4971-b3b7-43c5b03376a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# link to storage account\n",
    "storage_account_name = \"joindatasets\"\n",
    "container_name = \"datasets\"\n",
    "blob_url = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "\n",
    "strorage_account_key= dbutils.secrets.get(scope = \"key-vault-secret\", key = \"blob-datasets-accesskey\")\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n",
    "    strorage_account_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66fe0ed-4b4a-4f64-ad32-2054d37dacab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", True)\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22104bce-a177-4b33-a70e-f3c23adc13d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV from Azure Blob Storage\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "csv_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/department_data.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d3208f-dee1-45f1-86bf-52df20659fb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Join Big and Small table - SortMerge vs BroadCast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da4b732-9721-4c63-a58e-25d3448cfb96",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\") \\\n",
    "    .schema(_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(f\"{blob_url}/employee_records.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd414921-bd93-4a72-adba-de47070a68e3",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\") \\\n",
    "    .schema(_dept_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(f\"{blob_url}/department_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9e2155-8422-4274-b228-c6b784981650",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Join in noop format for performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c620377-716d-438d-a908-d63fce911c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join both datasets\n",
    "sc.setJobDescription(\"A: SortMerge Join\")\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "\n",
    "# df_joined.explain()\n",
    "# Write data in noop format for performance beachmarking\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86065497-1fd0-4e37-8e30-e66f9d2b7b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5e8903-6094-4343-9f29-de5d0bab7b87",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c768dd31-133f-4993-8159-29e98d03d517",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"B: Broadcast join\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5add926-803f-456a-b65d-ed6aecbb4f7e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9407bf4-f253-4050-9dfb-84631e4a6613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Join Big and Big table - SortMerge without Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc660fa3-0587-46a8-8528-c3aba698db62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\") \\\n",
    "    .schema(sales_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(f\"{blob_url}/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9459df-9121-446a-a126-00f3980451da",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\") \\\n",
    "    .schema(city_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(f\"{blob_url}/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473d36f4-7154-41b4-a17d-e003dc1d7410",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Data\n",
    "sc.setJobDescription(\"C: SortMerge Big Tables\")\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")\n",
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f976196-9596-4c4e-b4b7-a98cf6d3009c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explain Plan\n",
    "df_sales_joined.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74408606-b835-4ab9-a7c1-a4beaefb4b59",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "##### Write Sales and City data in Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c85eed-d0b2-4e20-b8a9-d65626329621",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS sales_bucket;\n",
    "DROP TABLE IF EXISTS city_bucket;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75446bc-2259-49ef-a76d-3bb4c317ab1f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write Sales data in Buckets\n",
    "sc.setJobDescription(\"D: create BucketLargeTables\")\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/sales_bucket.csv\").saveAsTable(\"sales_bucket\")\n",
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/city_bucket.csv\").saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dc8775f-9a2e-4c55-a8e8-43feaf693e57",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "#### Join Sales and City data - SortMerge with Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5527007a-3bb1-43e3-8a1b-2573b73e60cd",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join datasets\n",
    "sc.setJobDescription(\"Db: join BucketLargeTables\")\n",
    "\n",
    "#read bucket data\n",
    "sales_bucket = spark.read.table(\"sales_bucket\")\n",
    "city_bucket = spark.read.table(\"city_bucket\")\n",
    "\n",
    "#create join\n",
    "df_joined_bucket = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")\n",
    "\n",
    "#write data\n",
    "df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4dcdfb-0c9a-4b05-bf82-4579de0dd41c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_joined_bucket.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99b51fe-c985-41cb-9559-334adf4c60ad",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "#### Points to note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af49951-7a93-4937-83a1-6d7580e42ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Joining Column different than Bucket Column, Same Bucket Size - Shuffle on Both table\n",
    "2. Joining Column Same, One table in Bucket - Shuffle on non Bucket table\n",
    "3. Joining Column Same, Different Bucket Size - Shuffle on Smaller Bucket Side\n",
    "4. Joining Column Same, Same Bucket Size - No Shuffle (Faster Join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da6e1a37-d205-4f0e-9dfe-d1824629a70f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. So its very importatant to choose correct Bucket column and Bucket Size\n",
    "2. Decide effectively on number of Buckets, as too mant buckets with not enough data can lead to Small file issue.\n",
    "3. Datasets are Small - you can prefer Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c173743-da6d-4b67-9412-72be3a5a24aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728fc276-544e-4315-aa44-a9cfa3931d6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Suffle Hash Join\n",
    "Steps:\n",
    "\t- Both ds will be shuffled among the executors based on the key column\n",
    "\t- The smaller ds will be hashed in each executor in order to match with hashed  key of bigger ds\n",
    "\t- Once hashed key match, joining works\n",
    "\n",
    "Points:\n",
    "\t- Support al (equal) joins except full outer join\n",
    "\t- No sort steps, thus keys are not sorted\n",
    "\t- Suitable for joins where one smaller dataset which can fit in memory\n",
    "\t- To make sure spark used shuffle hash joins, we need to set:\n",
    "\t\tspark.sql.join.preferSortMergeJoin=false\n",
    "\t\t\n",
    "Sort merge join\n",
    "Steps:\n",
    "\t- Both ds will be shuffled among the executors based on the key column\n",
    "\t- The joining keys from both datasets will be sorted in same order\n",
    "\t- Once the joining key are sorted the merging happens (thus it sort merge)\n",
    "\n",
    "Points:\n",
    "\t- It supports all join types including full outer join\n",
    "\t- Since there is a sort step, it can be expensive join if not optimized properly\n",
    "\t- Preferred when we have two big datasets to join\n",
    "\t- We can set to use sort merge joins\n",
    "\t\tspark.sql.join.preferSortMergeJoin=true\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3133731004238510,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "optimizing_joins",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
