{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e424b8d-b41f-4e09-a67e-15bf525dccdf",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Skewness and Spillage\")\n",
    "    .master(\"spark://197e20b418a6:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec79c45-3b94-40b1-95c5-d60bc4a5e0b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c40955a-35e5-401f-85f5-0657a03a4b1e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4a4fde-e81e-4554-b1f9-5ebe6fae72ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# link to storage account\n",
    "storage_account_name = \"joindatasets\"\n",
    "container_name = \"datasets\"\n",
    "blob_url = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "\n",
    "strorage_account_key= dbutils.secrets.get(scope = \"key-vault-secret\", key = \"blob-datasets-accesskey\")\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n",
    "    strorage_account_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b7e1cf-dcc2-4b32-9877-8170c06ac41c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Employee data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38555d5f-7a99-44e2-94bd-e881f172c93d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "444b2be3-1f50-4e51-92b2-533ebd33efa5",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378277cc-f6e9-4a89-ac64-563307ee68d9",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f642229f-4013-45c5-aae0-f89a9f2cb4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Explain Plan\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba8c4e7-6ceb-40dd-8dcc-7745b93b1405",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "\n",
    "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1f712f0-6a20-4ede-b840-a105d5386b5c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e5b8ae-7cb6-413e-9951-c2ea92b65a1b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set shuffle partitions to a lesser number - 16\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2941f5a-f2a2-4526-8545-494878cb3263",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time and add to Employee as salt\n",
    "@udf\n",
    "def salt_udf():\n",
    "    return random.randint(0, 32)\n",
    "\n",
    "# Salt Data Frame to add to department\n",
    "salt_df = spark.range(0, 32)\n",
    "salt_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84264f47-85e7-4584-8d41-ac3046e05257",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salted Employee\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf()))\n",
    "\n",
    "salted_emp.show()                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ffba6c-72ab-4d99-a618-15ed2c1b2147",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salted Department\n",
    "\n",
    "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\"))\n",
    "\n",
    "salted_dept.where(\"department_id = 9\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b541560a-2c2f-438a-8616-80bcc4f0e184",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce6cfd4c-9d5b-4c55-9b9e-8f6516f4e65b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b648a3-8115-4c17-9ac3-c7abb54ed846",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04165a67-9af8-4ac2-a9f3-7ce1d755b5dd",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f171c4d-be23-4543-83b4-3b117af5026b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20_skewness_and_spillage",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
